{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuIQohHZkDI0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "brt_4p3bkU5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/main/en/quantization/quanto"
      ],
      "metadata": {
        "id": "X29xVMV-ow3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ü§ó optimum-quanto library is a versatile pytorch quantization toolkit. The quantization method used is the linear quantization. Quanto provides several unique features such as:\n",
        "\n",
        "weights quantization (float8,int8,int4,int2)\n",
        "activation quantization (float8,int8)\n",
        "modality agnostic (e.g CV,LLM)\n",
        "device agnostic (e.g CUDA,XPU,MPS,CPU)\n",
        "compatibility with torch.compile\n",
        "easy to add custom kernel for specific device\n",
        "supports quantization aware training"
      ],
      "metadata": {
        "id": "z6fR_cHGo0ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optimum-quanto accelerate transformers"
      ],
      "metadata": {
        "id": "WMDP2ZTAkU88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n",
        "\n",
        "model_id = \"facebook/opt-125m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "quantization_config = QuantoConfig(weights=\"int8\")\n",
        "quantized_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\", quantization_config=quantization_config)"
      ],
      "metadata": {
        "id": "n-0R6Lj-k3Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ŸáŸÑ ÿßŸÑŸÉŸàÿØ Ÿäÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑŸÖÿ∂ÿ∫Ÿàÿ∑\n",
        "\n",
        "ŸÜÿπŸÖÿå ÿßŸÑŸÉŸàÿØ Ÿäÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑŸÖÿ∂ÿ∫Ÿàÿ∑ (quantized model).\n",
        "\n",
        "ÿßŸÑÿ™ŸÅÿ≥Ÿäÿ±:\n",
        "\n",
        "ÿ™ŸÖ ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ QuantoConfig ŸÖÿπ weights=\"int8\". Ÿáÿ∞ÿß ŸäÿπŸÜŸä ÿ£ŸÜ ÿ£Ÿàÿ≤ÿßŸÜ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ™ŸÖ ÿ™ÿ≠ŸàŸäŸÑŸáÿß ÿ•ŸÑŸâ ÿ£ÿπÿØÿßÿØ ÿµÿ≠Ÿäÿ≠ÿ© ŸÖŸÜ 8 ÿ®ÿ™ÿå ŸÖŸÖÿß ŸäŸÇŸÑŸÑ ŸÖŸÜ ÿ≠ÿ¨ŸÖ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨"
      ],
      "metadata": {
        "id": "ukxrCbxOllvE"
      }
    },
    {
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(task=\"text-generation\", model=quantized_model, tokenizer=tokenizer)\n",
        "\n",
        "prompt = \"hi\"  # Replace with your desired prompt in Arabic\n",
        "generated_text = generator(prompt, max_length=50)[0]['generated_text']\n",
        "print(generated_text)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EBnXo7jFlYSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o2GxSXjVk3vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Li23C7uimt7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QB-AG-8Dmt-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5pzKdRzumuEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JcmK-jhMmuGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UVMy-3zdmuJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ÿ∂ÿ∫ÿ∑"
      ],
      "metadata": {
        "id": "lCOqqsStm5KU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n",
        "\n",
        "model_id = \"facebook/opt-125m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "quantization_config = QuantoConfig(weights=\"int2\")\n",
        "quantized_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\", quantization_config=quantization_config)"
      ],
      "metadata": {
        "id": "bF4MvoP1muZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ÿßÿ≥ÿ™ÿØŸÑÿßŸÑ ÿ®ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ®ÿπÿØ ÿ∂ÿ∫ÿ∑Ÿá"
      ],
      "metadata": {
        "id": "5MGmKNjWm7ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(task=\"text-generation\", model=quantized_model, tokenizer=tokenizer)\n",
        "\n",
        "prompt = \"hi\"  # Replace with your desired prompt in Arabic\n",
        "generated_text = generator(prompt, max_length=50)[0]['generated_text']\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "cfHFztovmzMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uvq6UGF_nMTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q-pCCk28nMQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IZHifpppnMN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SGxm5S7mnMCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dMBDQdtInMAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token XXXXXXXXXXXXXXX"
      ],
      "metadata": {
        "id": "qc-NRZtGnRDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ÿ¥ÿ∫ÿßŸÑ########################################"
      ],
      "metadata": {
        "id": "WJG0JNK1ptHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n",
        "import torch\n",
        "model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "quantization_config = QuantoConfig(weights=\"int2\")\n",
        "quantized_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\", quantization_config=quantization_config)"
      ],
      "metadata": {
        "id": "P-zJSrRSnRWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(task=\"text-generation\", model=quantized_model, tokenizer=tokenizer)\n",
        "\n",
        "prompt = \"hi\"  # Replace with your desired prompt in Arabic\n",
        "generated_text = generator(prompt, max_length=50)[0]['generated_text']\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "Oobb-BA9nRWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#######################################"
      ],
      "metadata": {
        "id": "39plEqofpwq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Qg4e6E_fpwgN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E0YSeTiAp3zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xCfF3KqMp3w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_0O6EqIvp3uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9n3uzv55p3q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "esAYlZUBp3oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1RdX9c0Rp3lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ayÿ¥ÿ∫ÿßŸÑ ÿ¨ŸäÿØ@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
      ],
      "metadata": {
        "id": "hMCAYiP9r053"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ValueError: Only support weights in ['float8', 'int8', 'int4', 'int2'] but found int3"
      ],
      "metadata": {
        "id": "1glER8pnqElt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n",
        "import torch\n",
        "model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "quantization_config = QuantoConfig(weights=\"int4\")\n",
        "quantized_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\", quantization_config=quantization_config)"
      ],
      "metadata": {
        "id": "M6aTgKRKp4Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(task=\"text-generation\", model=quantized_model, tokenizer=tokenizer)\n",
        "\n",
        "prompt = \"hi\"  # Replace with your desired prompt in Arabic\n",
        "generated_text = generator(prompt, max_length=50)[0]['generated_text']\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "fMbDyf01p4Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
      ],
      "metadata": {
        "id": "jemWkPxur4GF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FKHd4h4txZpw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}